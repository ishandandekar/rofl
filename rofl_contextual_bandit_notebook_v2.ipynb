{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ROFL contextual bandit demo \u2014 v2\n",
        "\n",
        "This notebook extends the previous version with:\n",
        "- Neural bootstrap ensemble contextual bandit\n",
        "- IPS-weighted regression trainer\n",
        "- Instructions to plug in DuckDB (with placeholders)\n",
        "- How to run the Streamlit visualizer app included in this folder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rofl_bandit import generate_synthetic_data, LinUCB, LinearThompson, NeuralBootstrapBandit, train_reward_model, OffPolicyEvaluator, ips_weighted_regression, predict_from_models\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic dataset\n",
        "data = generate_synthetic_data(n=4000, n_arms=5, d=8, seed=42)\n",
        "X = data['X']\n",
        "rewards = data['rewards']\n",
        "actions = data['actions']\n",
        "propensities = data['propensities']\n",
        "n_arms = data['true_thetas'].shape[0]\n",
        "\n",
        "print('Dataset sizes: n=', X.shape[0], 'd=', X.shape[1], 'n_arms=', n_arms)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train LinUCB, Linear Thompson, and NeuralBootstrapBandit (offline imitation)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d = X.shape[1]\n",
        "linucb = LinUCB(n_arms=n_arms, n_features=d, alpha=0.8)\n",
        "for i in range(X.shape[0]):\n",
        "    linucb.update(actions[i], X[i], rewards[i])\n",
        "lts = LinearThompson(n_arms=n_arms, n_features=d, v2=1.0, lambda_reg=1.0)\n",
        "for i in range(X.shape[0]):\n",
        "    lts.update(actions[i], X[i], rewards[i])\n",
        "neural = NeuralBootstrapBandit(n_arms=n_arms, n_features=d, n_models=6)\n",
        "neural.fit(X, actions, rewards)\n",
        "print('Trained linucb, lts (imitation), and neural ensemble')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Off-policy evaluation (IPS & DR) for all policies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "target_linucb = np.array([linucb.select_arm(X[i]) for i in range(X.shape[0])])\n",
        "target_lts = np.array([lts.select_arm(X[i]) for i in range(X.shape[0])])\n",
        "target_neural = np.array([neural.select_arm(X[i]) for i in range(X.shape[0])])\n",
        "\n",
        "q_hat = train_reward_model(X, actions, rewards, n_arms)\n",
        "ips_linucb = OffPolicyEvaluator.ips(rewards, actions, target_linucb, propensities, clip=100.0)\n",
        "ips_lts = OffPolicyEvaluator.ips(rewards, actions, target_lts, propensities, clip=100.0)\n",
        "ips_neural = OffPolicyEvaluator.ips(rewards, actions, target_neural, propensities, clip=100.0)\n",
        "dr_linucb = OffPolicyEvaluator.dr(rewards, actions, target_linucb, propensities, q_hat)\n",
        "dr_lts = OffPolicyEvaluator.dr(rewards, actions, target_lts, propensities, q_hat)\n",
        "dr_neural = OffPolicyEvaluator.dr(rewards, actions, target_neural, propensities, q_hat)\n",
        "\n",
        "pd.DataFrame({\n",
        "    'policy': ['linucb', 'linear_thompson', 'neural'],\n",
        "    'IPS': [ips_linucb, ips_lts, ips_neural],\n",
        "    'DR': [dr_linucb, dr_lts, dr_neural]\n",
        "})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IPS-weighted regression trainer (to learn a policy via offline weighted supervised learning)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "models = ips_weighted_regression(X, actions, rewards, propensities, n_arms)\n",
        "preds = predict_from_models(models, X)\n",
        "# derive greedy policy from preds\n",
        "greedy_from_ips = np.argmax(preds, axis=1)\n",
        "ips_ipsgreedy = OffPolicyEvaluator.ips(rewards, actions, greedy_from_ips, propensities, clip=100.0)\n",
        "dr_ipsgreedy = OffPolicyEvaluator.dr(rewards, actions, greedy_from_ips, propensities, q_hat)\n",
        "print('IPS-weighted regression policy evaluation')\n",
        "pd.DataFrame({'policy':['ips_weighted_regression'], 'IPS':[ips_ipsgreedy], 'DR':[dr_ipsgreedy]})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replace synthetic data with DuckDB\n",
        "\n",
        "Below is an example snippet to load your `claims_snapshot` from DuckDB and construct the required arrays. Edit column names and preprocessing to match your schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "'''",
        "import duckdb\n",
        "con = duckdb.connect('/path/to/your.duckdb')\n",
        "q = '''SELECT feature_1, feature_2, feature_3, /*...*/, action_taken AS action, reward, propensity FROM claims_snapshot WHERE date >= '2025-01-01' '''\n",
        "df = con.execute(q).fetchdf()\n",
        "feature_cols = ['feature_1','feature_2','feature_3']\n",
        "X = df[feature_cols].values\n",
        "actions = df['action'].astype(int).values\n",
        "rewards = df['reward'].values\n",
        "propensities = df['propensity'].values\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streamlit visualizer\n",
        "\n",
        "A Streamlit app `rofl_streamlit_app.py` is included next to this notebook. Run it with:\n",
        "```\n",
        "streamlit run rofl_streamlit_app.py\n",
        "```\n",
        "It will load either synthetic data or your DuckDB (edit path) and let you choose policies and display IPS/DR + action distributions using Altair.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}